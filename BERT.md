## 背景

- ​**时间背景**：2018年由Google提出，成为NLP领域里程碑式模型。

- ​**技术背景**：
	- 传统词嵌入（如Word2Vec）无法处理一词多义和上下文信息。
	- 预训练模型（如ELMo、GPT）开始兴起，但存在单向上下文限制（如GPT仅用左向上下文）。
	- Transformer架构（2017）为并行化处理序列提供了新思路。

- ​**核心目标**：通过双向预训练提取深层上下文特征，统一不同NLP任务的微调框架。

## 网络结构

### 预训练阶段

**输入表示**
- ​**Token Embedding**：WordPiece分词，处理未登录词（如“playing”拆分为“play”+“##ing”）。
- ​**Segment Embedding**：区分两个句子（如$E_A$和$E_B$），用于问答、推理任务。
- ​**Position Embedding**：学习位置编码，支持最长512个token的序列。
- ​**特殊标记**：`[CLS]`（分类标记）、`[SEP]`（句子分隔符）、`[MASK]`（掩码占位符）。

**预训练任务**
- **Masked Language Model (MLM)**：
	- 随机掩盖15%的token，其中：
		- 80%替换为`[MASK]`
		- 10%替换为随机词
		- 10%保留原词
   - 目标函数：$\mathcal{L}_{\text{MLM}} = -\sum \log P(x_i | x_{\backslash i})$

2. ​**Next Sentence Prediction (NSP)**：
   - 输入两个句子A和B，50%概率B是A的真实下一句。
   - 二分类任务，用`[CLS]`标记输出预测结果。

#### 训练细节
- ​**数据**：BooksCorpus（8亿词） + 英文维基百科（25亿词）
- ​**硬件**：16个TPU（共256芯片），训练约4天

### 微调阶段

#### 任务适配
- ​**分类任务**​（如情感分析）：取`[CLS]`标记的输出向量作为分类特征。
- ​**序列标注**​（如NER）：每个token的输出向量独立预测标签。
- ​**问答任务**​（如SQuAD）：输入格式为$[CLS] \text{Question} [SEP] \text{Context} [SEP]$，输出答案区间概率。

#### 参数调整
- 学习率通常比预训练阶段小（如预训练lr=$1e^{-4}$ vs 微调lr=$2e^{-5}$）
- 可在最后一层添加任务特定的网络层（如全连接分类器）

## 优点和缺点

**优点**
1. ​**双向上下文建模**：突破GPT的单向限制，捕捉左右侧上下文关系。
2. ​**任务无关性**：同一套预训练模型支持多种下游任务微调。
3. ​**长距离依赖处理**：Transformer的Self-Attention机制优于RNN/CNN。

**缺点**
1. ​**计算资源消耗大**：Base版需110M参数，Large版需340M参数。
2. ​**预训练-微调差异**：MLM中`[MASK]`标记在微调阶段不会出现，导致训练/推理不一致。
3. ​**长文本处理不足**：最大输入长度限制为512 token，不适合长文档任务。
4. ​**静态掩码策略**：预训练时对同一数据多次epoch使用相同掩码模式。

## 历史作用

1. ​**推动预训练范式**：确立“预训练+微调”为NLP任务标准流程。
2. ​**启发后续模型**：RoBERTa（优化训练策略）、ALBERT（参数共享）、DistilBERT（模型压缩）等均基于BERT改进。
3. ​**跨领域影响**：推动多模态模型（如VideoBERT、BioBERT）发展。
4. ​**工业界应用**：成为搜索引擎（如Google）、智能客服等场景的基准模型。