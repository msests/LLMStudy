## 1. 背景

在大语言模型（Large Language Model, LLM）训练中，混合精度训练（Mixed Precision Training）通过结合使用16位浮点数（FP16）和32位浮点数（FP32）计算，可以显著提升训练效率并降低显存占用。

## 2. 核心概念

### 2.1 精度类型对比

| 精度类型 | 存储大小 | 动态范围                 | 计算速度   | 显存占用 |
| ---- | ---- | -------------------- | ------ | ---- |
| FP32 | 32位  | $10^{-38}$~$10^{38}$ | 基准值    | 100% |
| FP16 | 16位  | $10^{-5}$~$10^{5}$   | 2-8x加速 | 50%  |

### 2.2 关键技术组件

混合精度训练需要autocast和GradScaler这两个类。

#### 2.2.1 torch.cuda.amp.autocast

- 自动混合精度上下文管理器。
- 自动将支持的操作转为 **FP16（半精度）** 执行（比如矩阵乘法、卷积），提高性能和减少显存。
- 保留数值敏感操作为 **FP32**，例如 softmax、归一化、某些 loss 运算，避免数值精度丢失。

**使用方式**：

```python
with autocast():
	outputs = model(inputs)
	loss = loss_fn(outputs, targets)
```

- 说明：
	- 只在 `with autocast()` 块内启用自动精度切换。
	- 多数算子自动选择 FP16 或 FP32，无需手动控制。

#### 2.2.2 torch.cuda.amp.GradScaler

- 处理梯度缩放，避免数值下溢。
- 问题来源：
	- FP16 的表示范围较小，**梯度值容易下溢为 0**，导致模型无法学习。
- 作用：
	- 在反向传播前将 loss 放大（scale），让梯度也变大，**避免下溢**。
	- 在 optimizer.step() 前再按比例缩回来。
	- 自动检测是否出现 NaN/Inf，必要时降低 scale 值重试。

**使用方式**：
```python
scaler = GradScaler()  
# 1. 放大 loss，反向传播 
scaler.scale(loss).backward()  
# 2. optimizer.step() 包装 
scaler.step(optimizer)  
# 3. 每轮更新 scale 值 
scaler.update()
```

## 3. 实现原理

梯度缩放公式：
$$ \text{scaled\_loss} = \text{loss} \times \text{scale\_factor} $$
$$ \text{scaled\_grad} = \nabla \text{scaled\_loss} \times \frac{1}{\text{scale\_factor}} $$

## 4. PyTorch实现示例

```python
import torch
from torch.cuda.amp import autocast, GradScaler

model = YourLLMModel()
optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4)
scaler = GradScaler()

for inputs, labels in dataloader:
    optimizer.zero_grad()
    
    with autocast():
        outputs = model(inputs)
        loss = loss_function(outputs, labels)
    
    scaler.scale(loss).backward()
    scaler.step(optimizer)
    scaler.update()
```

## 5. 性能对比

| 训练模式    | 速度提升 | 显存节省 | 模型精度  |
| ------- | ---- | ---- | ----- |
| FP32纯精度 | 基准值  | 0%   | 100%  |
| AMP混合精度 | 2.3x | 40%  | 99.8% |
| FP16纯精度 | 3.1x | 50%  | 97.2% |

## 6. 优点和缺点

### 6.1 优点：

 - 更快的训练速度
	 - FP16 运算在支持的硬件上（如 NVIDIA Volta/Turing/Ampere 架构 GPU）比 FP32 更快，尤其在 Tensor Cores 上。
- 显存占用更少
	- FP16 数据只占用 FP32 一半的内存，可以训练更大的 batch 或更大的模型。
- 提升吞吐量
	- 减少了内存带宽瓶颈，更多计算单元能同时工作，从而提升每秒训练样本数。
- 兼容大多数网络结构
	- 使用自动混合精度（如 PyTorch 的 AMP），不需要手动调整每一层的精度设置。

### 6.2 缺点：

- 数值不稳定
	- FP16 有更小的表示范围和精度，容易出现溢出或下溢，尤其是在梯度较小时。需要借助 `GradScaler` 来避免梯度消失。
- 部分操作不支持 FP16
	- 某些操作（如 `Softmax`、`BatchNorm` 等）在 FP16 下精度不够或不稳定，需要强制使用 FP32。
- 调试更困难
	- 出现梯度爆炸、loss 为 NaN 等问题时，混合精度训练可能更难定位原因。