## 背景

GPT-3（Generative Pre-trained Transformer 3）由OpenAI于2020年发布，是自然语言处理（NLP）领域的重要里程碑。它基于Transformer架构，通过大规模无监督预训练实现语言生成和理解。与前代模型（如GPT-2）相比，GPT-3显著提升了模型规模和通用性，推动了对“大模型”能力的探索，并引发了对通用人工智能（AGI）的讨论。

## 参数

1. **GPT-3 Small**:
    - 参数数量：约1.25亿（125M）
    - 层数：12
    - 注意力头数：12
    - 隐藏层维度：768
2. ​**GPT-3 Medium**:
    - 参数数量：约3.55亿（355M）
    - 层数：24
    - 注意力头数：16
    - 隐藏层维度：1024
3. ​**GPT-3 Large**:
    - 参数数量：约7.74亿（774M）
    - 层数：36
    - 注意力头数：20
    - 隐藏层维度：1280
4. ​**GPT-3 XL**:
    - 参数数量：约15.5亿（1.55B）
    - 层数：48
    - 注意力头数：25
    - 隐藏层维度：1600
5. ​**GPT-3 6.7B**:
    - 参数数量：约67亿（6.7B）
    - 层数：96
    - 注意力头数：32
    - 隐藏层维度：4096
6. ​**GPT-3 13B**:
    - 参数数量：约130亿（13B）
    - 层数：128
    - 注意力头数：40
    - 隐藏层维度：5120
7. ​**GPT-3 175B**:
    - 参数数量：约1750亿（175B）
    - 层数：96
    - 注意力头数：96
    - 隐藏层维度：12288

## 亮点
1. ​**上下文学习**：无需微调即可通过输入示例（in-context learning）完成新任务  
2. ​**少样本/零样本学习**：仅需$k=0-64$个样本即可适应新场景  
3. ​**多模态扩展潜力**：后续研究基于GPT-3实现了代码生成（Codex）和图文结合（DALL·E）  
4. ​**注意力机制创新**：稀疏注意力（Sparse Attention）提升长文本处理效率