## 背景

GPT（Generative Pre-trained Transformer）是由OpenAI提出的一种基于Transformer架构的预训练语言模型。它代表了自然语言处理（NLP）领域的一项重大突破，尤其是在无监督预训练和迁移学习方面。GPT的设计受到ELMo和ULMFiT等模型的启发，但通过引入Transformer架构和大规模无监督预训练，GPT显著提升了语言模型的性能和通用性，成为现代NLP研究的重要里程碑。

## 网络结构

GPT的网络结构主要基于Transformer的解码器部分。Transformer由Vaswani等人在2017年提出，最初用于机器翻译任务，而GPT对其解码器模块进行了调整以适应语言建模需求。GPT1到GPT3在架构方面区别并不大，仅仅增加了网络的规模。

![](GPT.png)
## 亮点

GPT在设计和应用上具有以下几个显著亮点：

1. **无监督预训练**：利用大规模无标注文本进行预训练，GPT学习到丰富的语言表示，可迁移至多种NLP任务，如分类、问答等。
2. **单向语言模型**：采用自回归方式，仅基于前文预测当前词，使其在文本生成任务中表现优异。
3. **Transformer架构**：相比RNN或CNN，Transformer更擅长处理长距离依赖，并具有更高的并行计算效率。
4. **迁移学习能力**：通过预训练和微调，GPT能高效适应下游任务，降低对任务特定数据的需求。