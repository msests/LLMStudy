## **概念**  

**Paged Attention** 是一种改进的注意力机制，灵感来源于计算机系统中的**分页内存管理**（Paged Memory Management）。它主要用于解决大语言模型（LLM）在处理长序列时面临的高显存占用和计算效率问题。通过将注意力计算中的键（Key）和值（Value）缓存分割成更小的“页”（Pages），Paged Attention 可以更高效地管理显存，并支持灵活的内存共享，从而提升推理和训练的吞吐量。 

该技术由 **vLLM**（一个高效LLM推理框架）提出，并成功应用于实际推理优化中，显著提升了长文本生成的效率。  

## **原理**  

Paged Attention 的核心思想是**分块存储和动态加载**注意力机制的 KV Cache（键值缓存），其工作流程如下：  

1. **KV Cache 分页存储**  
   - 传统的注意力机制在解码时，所有历史 Token 的 Key 和 Value 都存储在连续显存中，导致显存浪费（如 Padding 占位）和碎片化问题。  
   - Paged Attention 将 KV Cache 划分为固定大小的“页”（例如 16 个 Token/页），并按需分配物理显存，类似于操作系统的分页机制。  

2. **按需加载与内存共享**  
   - 在计算注意力时，仅加载(Commit)当前查询（Query）所需的 KV 页，而非整个序列，减少显存占用。  
   - 支持不同序列间的内存共享（例如并行生成多个序列时，共享相同前缀的 KV Cache），提高显存利用率。  

3. **高效注意力计算**  
   - 结合分页存储，Paged Attention 在计算 Softmax 时仅访问相关的 KV 页，避免冗余计算，提升效率。  

## **优点和缺点**  

### 优点

- **显存利用率高**
	- 通过分页管理，减少显存碎片，支持更长的上下文窗口。  
- **吞吐量提升**
	- 在批量推理时，显存共享机制可显著提高并发处理能力（vLLM 实测提升 10 倍以上）。  
- **兼容性强**
	- 无需修改模型架构，可无缝适配现有 Transformer 模型（如 GPT、LLaMA 等）。  

### 缺点

 - **实现复杂度高**
	 - 需维护页表、处理动态内存分配，增加系统设计难度。  
- **小规模数据可能不显著**
	- 短文本推理场景下，分页管理的优势有限。  
- **潜在访存开销**
	- 频繁的页加载可能引入额外延迟（但通常被整体效率提升抵消）。  