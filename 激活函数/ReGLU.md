## 1. 概述

**ReGLU**（Rectified Gated Linear Unit）是一种改进的门控线性单元，结合了**ReLU激活函数**和**门控机制**。它通过引入非线性与动态特征选择能力，常见于现代神经网络架构（如Transformer的FFN层变体）。其核心思想是通过门控信号控制特征流的权重，增强模型的表达能力。

## 2. 数学原理与特性

### 数学定义

给定输入向量 $x \in \mathbb{R}^d$，ReGLU 的计算分为两步：

1. **线性投影**：将 $x$ 分别映射到两个子空间：
$$
A = xW + b, \quad B = xV + c
$$
   其中 $W, V \in \mathbb{R}^{d \times h}$ 是权重矩阵，$b, c \in \mathbb{R}^h$ 是偏置项。
   
2. **门控操作**：对 $A$ 应用 ReLU 激活函数，再与 $B$ 逐元素相乘：
$$
\text{ReGLU}(x) = \text{ReLU}(A) \odot B = \max(0, xW + b) \odot (xV + c)
$$
   其中 $\odot$ 表示逐元素乘法。

## 3. 优点与缺点

### 3.1 优点

- **ReLU的优点**
	- ReGLU基于ReLU因此拥有ReLU的优点。
- **动态门控机制**
	- 门控信号 $B$ 根据输入动态调整特征权重，增强模型灵活性。

### 3.2 缺点

- **ReLU的缺点**
- **门控偏差**
	- 门控信号 $B$ 的幅值无界，可能导致训练不稳定（需配合归一化层使用）。
- **参数翻倍**
	- 需要维护两组权重矩阵 $W$ 和 $V$，参数量为普通全连接层的两倍。
