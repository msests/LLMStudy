## 概述

ALiBi（Attention with Linear Biases）是一种用于Transformer模型的**相对位置编码方法**，由Press等人于2022年提出。其核心思想是通过在注意力机制中引入线性偏置项，使模型能够隐式地学习位置关系。相比传统的位置编码，ALiBi在长文本处理和外推性方面展现出了更好的性能。

## 数学原理

### 基本公式

给定查询向量$q_i$和键向量$k_j$，ALiBi算法修改后的注意力分数为：
$$a_{i,j} = q_i^T k_j + b_{i,j}$$
其中：
- $i$: $Q$向量位置。
- $j$: $K$向量位置。
- $m$: 负斜率参数，可以是固定值也可以是可学习参数，效率影响并不大。

### 多头扩展

在实际实现中，不同注意力头使用不同的斜率$m$：
$$m_h = \frac{1}{2^{8h/H}}$$
其中：
- $h$: 头索引（0 ≤ h < H）
- $H$: 总头数

如果$m$是可学习参数，用该值来初始化参数。

### 相对距离计算

对于二维注意力矩阵，计算公式扩展为：
$$b_{i,j} = -m \cdot |i - j| \quad i > j$$
因此距离越近的输入，点积的结果越大（因为减去的值越小），模型越关注，而远距离位置的注意力权重随距离线性衰减。

![](ALiBi.drawio.svg)

### 完整注意力计算

最终注意力权重：
$$
\text{Attention}(Q,K,V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}} + B\right)V
$$
其中$B$是包含所有$b_{i,j}$的偏置矩阵。

### 梯度分析

如果$m$可以学习，则$m$的梯度更新过程：
- 对偏置项的梯度计算：$\frac{\partial L}{\partial m} = \sum_{i,j} \frac{\partial L}{\partial a_{i,j}} \cdot |i-j|$
- 斜率参数$m$通过反向传播自动学习。

## 优点与缺点

### 优点

- **外推能力强**
	- 传统绝对位置编码在遇到超出训练预设的长度时（比如512），需要采取随机位置编码或截断，外推性能较差。
	- 相对距离计算不依赖绝对的位置索引，能处理比训练时更长的序列，对长文本友好。
- **计算高效**
	- 偏置矩阵可以预先计算并缓存，不需要显示的计算位置嵌入。
	- 相比相对位置编码，没有额外训练参数（或者只有一个参数$m$），降低了模型复杂度。
- **兼容性强**
	- 可与绝对位置编码结合使用。

### 缺点

- **固定衰减模式**
	- 线性惩罚可能不适合所有任务
- **长程依赖限制**
	- 长距离注意力权重衰减可能影响远距离关系的建模。
- **方向不敏感**
	- 偏置只基于相对位置，未考虑方向等位置关系。