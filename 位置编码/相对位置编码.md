## 概述

Transformer-XL 针对传统Transformer无法建模长距离依赖的问题，提出了**相对位置编码**机制。传统绝对位置编码在计算注意力时会将位置信息与词向量耦合，导致跨片段推理困难。相对位置编码通过**引入相对位置偏置**，使模型更灵活地捕捉token间的相对距离关系，同时支持超长上下文建模。

## 数学原理

### 传统注意力计算回顾

在标准Transformer中，查询矩阵$Q$和键矩阵$K$的计算包含位置编码：
$$
Q = (W^Q (E_x + U)), \quad K = (W^K (E_x + U))
$$
其中$E_x$为词嵌入，$U$为绝对位置编码

注意力得分计算为：
$$
A_{i,j} = \text{softmax}\left( \frac{Q_i K_j^T}{\sqrt{d}} \right)
$$

### 相对位置编码改进

1. **分解位置信息**：将绝对位置编码分解为相对位置偏置
2. **重新参数化注意力得分**：将位置信息表示为相对距离的函数

具体推导过程：
1. 将注意力得分分解为四项：
$$
A_{i,j} = \underbrace{E_x^T W_q^T W_k E_x}_{(a)} + \underbrace{E_x^T W_q^T W_k U_j}_{(b)} + \underbrace{U_i^T W_q^T W_k E_x}_{(c)} + \underbrace{U_i^T W_q^T W_k U_j}_{(d)}
$$

2. 引入相对位置变量$R_{i-j}$代替$U_j$，其中$R$为相对位置编码矩阵：
   - 用可训练参数$u,v \in \mathbb{R}^d$替换绝对位置项
   - 拆分权重矩阵$W_k$为$W_{k,E}$和$W_{k,R}$

最终改进后的注意力得分：
$$
A_{i,j} = \frac{
    \underbrace{E_x^T W_q^T W_{k,E} E_x}_{内容-内容项} + 
    \underbrace{E_x^T W_q^T W_{k,R} R_{i-j}}_{内容-位置项} + 
    \underbrace{u^T W_{k,E} E_x}_{全局内容偏置} + 
    \underbrace{v^T W_{k,R} R_{i-j}}_{全局位置偏置}
}{\sqrt{d}}
$$

3. 相对位置编码矩阵$R$的生成：
$$
R_{pos} = [\cdots; \sin(pos/10000^{2k/d}); \cos(pos/10000^{2k/d}); \cdots]
$$
其中$pos$为最大相对距离，通常设置为预设值（如512）

## 优点和缺点

### 优点

- **长度外推性**
	- 支持处理比训练时更长的序列
- **上下文连续性**
	- 通过记忆机制实现跨片段的位置信息传递
- **计算高效**
	- 相对位置编码可预计算并复用

###  缺点

- **实现复杂度**
	- 需要维护相对位置矩阵和多个参数子空间。
- **内存消耗**
	- 保存相对位置偏置需要额外显。
- **长程衰减**
	- 极端远距离位置的相对编码区分度降低。