## 位置编码概念

位置编码是为序列中每个**位置**（token的索引）生成一个向量表示的技术，用于向模型注入序列的**顺序信息**。由于Transformer的自注意力机制本身是置换等变的（permutation-equivariant），计算token相似度时，会丢失原始序列的位置信息，无法区分不同的输入顺序。因此需要通过位置编码显式引入位置信息。

> 即使没有位置编码，模型也能隐式的学习到微弱的位置信息，这些信息主要来源于单词的统计学规律而非显示的位置信息，效率并不高。

## 位置编码的指标

- 表达能力
	- 能否有效编码绝对位置、相对位置及距离信息。
- 长度外推能力
	- 对超过训练时最大长度的序列是否仍能有效工作
- 计算效率
	- 计算复杂度是否与序列长度呈线性/次线性关系

## 常见位置编码方法

### 绝对位置编码

- **Sinusoidal编码**（原始Transformer）：
$$PE_{(pos,2i)} = \sin(pos/10000^{2i/d})$$
$$PE_{(pos,2i+1)} = \cos(pos/10000^{2i/d})$$
  特点：确定性强、无需学习参数，但外推能力有限

- **可学习编码**（如BERT）：
$$PE_{pos} = E_{pos}, \quad E \in \mathbb{R}^{max\_len \times d}$$
  特点：通过训练获得位置向量，但难以泛化到未见过的位置。

### 相对位置编码

- **Transformer-XL式**：
  计算query $q_i$和key $k_j$的注意力分数时引入相对位置偏移：
$$A_{i,j} = q_i^T k_j + q_i^T r_{i-j}$$

- **旋转位置编码（RoPE）**（如LLaMA）：
  通过旋转矩阵将位置信息融入query/key：
$$q_m = f_q(x_m, m) = R_{\theta,m}W_q x_m$$
$$k_n = f_k(x_n, n) = R_{\theta,n}W_k x_n$$
  其中$R_{\theta,m}$是旋转矩阵，$\theta$为预设频率参数

### 其他方法

- **ALiBi**（Attention with Linear Biases）：
  在注意力分数中直接添加线性偏置：
$$A_{i,j} = q_i^T k_j - m \cdot |i-j|$$
  其中$m$是预设的斜率系数，随注意力头数递减
  特点：显著提升外推能力

- **T5 Bias**：
  将位置信息编码为注意力偏置矩阵：
$$A = QK^T + B$$
  其中$B$为可学习的相对位置偏置矩阵